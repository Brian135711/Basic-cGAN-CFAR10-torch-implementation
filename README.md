This is a basic implementation of a conditional generative adversarial network (cGAN) to facilitate learning for me and an opportunity for anyone who would like to contribute. A generator generates images based on random noise as a normal GAN but also takes in a vector of 10 class labels from the CFAR10 dataset ['horse', 'dog', ..., 'cat']. These labels are converted into an embedding before insertion into the generator and discriminator.  The discriminator judges real or fake and updates the weights of the generator as a normal GAN, except the idea is that learning and generation can be controlled by these labels as opposed to a normal GAN where the user has no control over what type of image is generated. Here is a link to the original paper Conditional Generative Adversarial Nets by Mirza and Osindero https://arxiv.org/abs/1411.1784; the notebook version works better and is based mainly on https://github.com/bhiziroglu/Conditional-Generative-Adversarial-Network 

The resolution of the original images seems to limit the ability to produce clear images. The basic network structure probably slows the time to convergence but makes the code somewhat more straightforward to digest. Possible areas for improvement might also be how the embedding process is implemented. It is a good starting point for learning, but I will be looking at other, more current methods. Increase batch size to 1000 and increase lr to .0003 on both generator and discriminator for better performance on the notebook.

